<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Perspective Tensor</title>
    <link>https://entropy-warrior.github.io/posts/</link>
    <description>Recent content in Posts on Perspective Tensor</description>
    <generator>Hugo -- 0.127.0</generator>
    <language>en-us</language>
    <atom:link href="https://entropy-warrior.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ðŸ‘‹ Welcome to Lin&#39;s blog!</title>
      <link>https://entropy-warrior.github.io/posts/landing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/landing/</guid>
      <description>I hope you enjoy your visit. This blog is all about my journey of lifelong learning and shifting perspectives. As an &amp;ldquo;entropy warrior&amp;rdquo;, I love finding order and meaning in the chaos. Dive in with me as I explore data science, AI, and career growth, using first principles to see the world in new ways.</description>
    </item>
    <item>
      <title>From Bird Flocks to Intelligence: The Power of Emergence</title>
      <link>https://entropy-warrior.github.io/posts/emergence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/emergence/</guid>
      <description>(An image from Xavi Bou&amp;rsquo;s project Ornitographies.)
Have you ever found bird flocks mesmerizing? I certainly do. But did you know that it&amp;rsquo;s not the birds that memorize, but the flock itself? This behavior arises from simple rules and interactions. It&amp;rsquo;s fascinating how a system with simple components and rules can create something incredibly complex and sophisticated. I find this theme absolutely captivating and wanted to share it with you.</description>
    </item>
    <item>
      <title>Life Lessons from Machine Learning</title>
      <link>https://entropy-warrior.github.io/posts/lifelesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/lifelesson/</guid>
      <description>Machines learn from us, we learn from their learnings.</description>
    </item>
    <item>
      <title>Modeling and Compression</title>
      <link>https://entropy-warrior.github.io/posts/compression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/compression/</guid>
      <description>Think Your Model is Smart? It&amp;rsquo;s Actually Just Really Good at Compression Data scientists are the modern-day wizards, conjuring insights from chaos. But have you ever considered that these magical models are essentially performing a high-tech form of data compression?
Let&amp;rsquo;s break it down.
Models: The Executive Summary of Reality Imagine cramming a semester&amp;rsquo;s worth of history into a bite-sized executive summary. That&amp;rsquo;s the essence of modeling in the world of data science.</description>
    </item>
    <item>
      <title>State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling</title>
      <link>https://entropy-warrior.github.io/posts/mamba/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/mamba/</guid>
      <description>Have you ever wondered how AI models make sense of sequential data like text, time series, or medical images as time series? Beside the &amp;ldquo;traditional&amp;rdquo; recurrent neural network, two powerful machine learning architectures recently emerged as frontrunners in this field: transformers and State Space Models (SSM). But what sets them apart, and how do they compare? Let&amp;rsquo;s delve into sequence modeling and examine the nuanced differences between these two &amp;ldquo;newcomers&amp;rdquo;.</description>
    </item>
  </channel>
</rss>
